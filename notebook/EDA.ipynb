{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Lectura del dataset\n",
    "df = pd.read_csv('../data/teleCust1000t.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificación de variables categóricas y numéricas\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar información básica\n",
    "print(\"Columnas categóricas:\")\n",
    "for col in categorical_columns: # type: ignore\n",
    "    print(f\"{col}: {df[col].nunique()} valores únicos\")\n",
    "\n",
    "print(\"\\nColumnas numéricas:\")\n",
    "for col in numerical_columns: # type: ignore\n",
    "    print(f\"{col}: rango [{df[col].min()}, {df[col].max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información detallada del DataFrame\n",
    "print(\"\\nInformación del DataFrame:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas básicas\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de Distribución de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración básica de visualización\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Variables numéricas a analizar\n",
    "numerical_vars = ['tenure', 'age', 'income', 'employ', 'address', 'reside']\n",
    "\n",
    "# Crear figura para histogramas y boxplots\n",
    "fig, axes = plt.subplots(len(numerical_vars), 2, figsize=(15, 24))\n",
    "fig.suptitle('Distribución de Variables Numéricas', fontsize=16, y=0.95)\n",
    "\n",
    "# Función para calcular estadísticas de asimetría y curtosis\n",
    "def get_distribution_stats(data):\n",
    "    return pd.Series({\n",
    "        'Media': np.mean(data),\n",
    "        'Mediana': np.median(data),\n",
    "        'Desv. Est.': np.std(data),\n",
    "        'Asimetría': stats.skew(data),\n",
    "        'Curtosis': stats.kurtosis(data)\n",
    "    })\n",
    "\n",
    "# Diccionario para almacenar estadísticas\n",
    "stats_dict = {}\n",
    "\n",
    "# Crear histogramas y boxplots\n",
    "for idx, var in enumerate(numerical_vars):\n",
    "    # Histograma\n",
    "    sns.histplot(data=df, x=var, kde=True, ax=axes[idx, 0])\n",
    "    axes[idx, 0].set_title(f'Histograma de {var}')\n",
    "    axes[idx, 0].set_xlabel(var)\n",
    "    axes[idx, 0].set_ylabel('Frecuencia')\n",
    "\n",
    "    # Boxplot\n",
    "    sns.boxplot(data=df, y=var, ax=axes[idx, 1])\n",
    "    axes[idx, 1].set_title(f'Boxplot de {var}')\n",
    "\n",
    "    # Calcular estadísticas\n",
    "    stats_dict[var] = get_distribution_stats(df[var])\n",
    "\n",
    "# Ajustar el layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar las gráficas\n",
    "plt.show()\n",
    "\n",
    "# Crear tabla de estadísticas\n",
    "stats_df = pd.DataFrame(stats_dict)\n",
    "print(\"\\nEstadísticas de distribución:\")\n",
    "print(stats_df.round(2))\n",
    "\n",
    "# Calcular y mostrar percentiles\n",
    "percentiles = df[numerical_vars].quantile([0.25, 0.5, 0.75])\n",
    "print(\"\\nPercentiles principales:\")\n",
    "print(percentiles.round(2))\n",
    "\n",
    "# Test de normalidad\n",
    "print(\"\\nTest de Normalidad (Shapiro-Wilk):\")\n",
    "for var in numerical_vars:\n",
    "    stat, p_value = stats.shapiro(df[var])\n",
    "    print(f\"{var}: p-value = {p_value:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones de Variables Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Crear una copia del dataframe para las transformaciones\n",
    "df_transformed = df.copy()\n",
    "\n",
    "# 1. Transformación logarítmica para income (debido a su alta asimetría)\n",
    "df_transformed['income_log'] = np.log1p(df['income'])  # log1p para manejar valores 0\n",
    "\n",
    "# 2. Aplicar RobustScaler a las variables con outliers\n",
    "robust_scaler = RobustScaler()\n",
    "variables_to_robust_scale = ['employ', 'address']\n",
    "df_transformed[['employ_scaled', 'address_scaled']] = robust_scaler.fit_transform(df[variables_to_robust_scale])\n",
    "\n",
    "# 3. Aplicar StandardScaler a variables más normales\n",
    "standard_scaler = StandardScaler()\n",
    "variables_to_standard_scale = ['tenure', 'age', 'reside']\n",
    "df_transformed[[f'{col}_scaled' for col in variables_to_standard_scale]] = standard_scaler.fit_transform(df[variables_to_standard_scale])\n",
    "\n",
    "# Configuración de la visualización\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Crear visualización comparativa\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 20))\n",
    "fig.suptitle('Comparación de Distribuciones: Original vs Transformada', fontsize=16)\n",
    "\n",
    "# 1. Comparar distribución de income\n",
    "sns.histplot(data=df, x='income', kde=True, ax=axes[0,0])\n",
    "axes[0,0].set_title('Income Original')\n",
    "axes[0,0].set_xlabel('Income')\n",
    "\n",
    "sns.histplot(data=df_transformed, x='income_log', kde=True, ax=axes[0,1])\n",
    "axes[0,1].set_title('Income (Log Transformada)')\n",
    "axes[0,1].set_xlabel('Log Income')\n",
    "\n",
    "# 2. Comparar variables con RobustScaler\n",
    "sns.boxplot(data=df[variables_to_robust_scale], ax=axes[1,0])\n",
    "axes[1,0].set_title('Variables Originales (Employ, Address)')\n",
    "\n",
    "sns.boxplot(data=df_transformed[['employ_scaled', 'address_scaled']], ax=axes[1,1])\n",
    "axes[1,1].set_title('Variables con RobustScaler')\n",
    "\n",
    "# 3. Comparar variables con StandardScaler\n",
    "sns.boxplot(data=df[variables_to_standard_scale], ax=axes[2,0])\n",
    "axes[2,0].set_title('Variables Originales (Tenure, Age, Reside)')\n",
    "\n",
    "sns.boxplot(data=df_transformed[[f'{col}_scaled' for col in variables_to_standard_scale]], ax=axes[2,1])\n",
    "axes[2,1].set_title('Variables con StandardScaler')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar estadísticas de las variables transformadas\n",
    "print(\"\\nEstadísticas de las variables transformadas:\")\n",
    "transformed_stats = df_transformed[[\n",
    "    'income_log',\n",
    "    'employ_scaled', 'address_scaled',\n",
    "    'tenure_scaled', 'age_scaled', 'reside_scaled'\n",
    "]].describe()\n",
    "print(transformed_stats)\n",
    "\n",
    "# Verificar normalidad de las variables transformadas\n",
    "print(\"\\nTest de Normalidad (Shapiro-Wilk) para variables transformadas:\")\n",
    "for column in transformed_stats.columns:\n",
    "    stat, p_value = stats.shapiro(df_transformed[column].dropna())\n",
    "    print(f\"{column}: p-value = {p_value:.2e}\")\n",
    "\n",
    "# Guardar las correlaciones de las variables transformadas\n",
    "correlation_matrix = df_transformed[[\n",
    "    'income_log',\n",
    "    'employ_scaled', 'address_scaled',\n",
    "    'tenure_scaled', 'age_scaled', 'reside_scaled'\n",
    "]].corr()\n",
    "\n",
    "print(\"\\nMatriz de correlaciones de variables transformadas:\")\n",
    "print(correlation_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento Avanzado de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 2. Análisis de balance de clases (ANTES del preprocesamiento)\n",
    "print(\"\\n2. ANÁLISIS DE BALANCE DE CLASES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calcular distribución de clases\n",
    "class_distribution = df['custcat'].value_counts()\n",
    "class_percentages = df['custcat'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Visualización del balance de clases\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Gráfico de barras\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=class_distribution.index, y=class_distribution.values)\n",
    "plt.title('Distribución de Clases (Conteo)')\n",
    "plt.xlabel('Categoría de Cliente')\n",
    "plt.ylabel('Número de Clientes')\n",
    "\n",
    "# Subplot 2: Gráfico de pie\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(class_distribution, labels=class_distribution.index, autopct='%1.1f%%')\n",
    "plt.title('Distribución de Clases (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimir estadísticas de balance\n",
    "print(\"\\nEstadísticas de balance de clases:\")\n",
    "for clase, (conteo, porcentaje) in enumerate(zip(class_distribution, class_percentages), 1):\n",
    "    print(f\"Clase {clase}: {conteo} clientes ({porcentaje:.1f}%)\")\n",
    "\n",
    "# 3. Preprocesamiento de datos\n",
    "print(\"\\n3. PREPROCESAMIENTO DE DATOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Crear copia para preprocesamiento\n",
    "df_processed = df.copy()\n",
    "\n",
    "# 3.1 Transformaciones numéricas\n",
    "print(\"\\nAplicando transformaciones...\")\n",
    "\n",
    "# Transformación logarítmica para income\n",
    "df_processed['income_log'] = np.log1p(df_processed['income'])\n",
    "\n",
    "# Crear características derivadas\n",
    "df_processed['income_per_year_employed'] = df_processed['income'] / (df_processed['employ'] + 1)\n",
    "df_processed['stability_score'] = df_processed['address'] * df_processed['tenure'] / (df_processed['age'] - 18)\n",
    "\n",
    "# Aplicar RobustScaler a variables con outliers\n",
    "robust_scaler = RobustScaler()\n",
    "robust_vars = ['employ', 'address', 'income_per_year_employed']\n",
    "df_processed[['employ_scaled', 'address_scaled', 'income_year_scaled']] = \\\n",
    "    robust_scaler.fit_transform(df_processed[robust_vars])\n",
    "\n",
    "# Aplicar StandardScaler a variables más normales\n",
    "standard_scaler = StandardScaler()\n",
    "standard_vars = ['tenure', 'age']\n",
    "df_processed[['tenure_scaled', 'age_scaled']] = \\\n",
    "    standard_scaler.fit_transform(df_processed[standard_vars])\n",
    "\n",
    "# Crear variables dummy para region\n",
    "df_processed = pd.get_dummies(df_processed, columns=['region'], prefix='region')\n",
    "\n",
    "# 4. Selección de características finales\n",
    "print(\"\\n4. SELECCIÓN DE CARACTERÍSTICAS Y ANÁLISIS FINAL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "features_to_use = [\n",
    "    'income_log', 'employ_scaled', 'address_scaled',\n",
    "    'tenure_scaled', 'age_scaled', 'income_year_scaled',\n",
    "    'stability_score', 'region_1', 'region_2', 'region_3'\n",
    "]\n",
    "\n",
    "# Preparar X e y\n",
    "X = df_processed[features_to_use]\n",
    "y = df_processed['custcat']\n",
    "\n",
    "# 5. Análisis de correlaciones\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Visualización de correlaciones\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Matriz de Correlación de Variables Procesadas')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Estadísticas finales\n",
    "print(\"\\nEstadísticas de las características procesadas:\")\n",
    "print(X.describe().round(3))\n",
    "\n",
    "# 7. Resumen de transformaciones\n",
    "print(\"\\nResumen de transformaciones aplicadas:\")\n",
    "print(\"1. Transformaciones logarítmicas:\")\n",
    "print(\"   - income_log\")\n",
    "print(\"\\n2. Variables escaladas con RobustScaler:\")\n",
    "print(\"   - employ_scaled\")\n",
    "print(\"   - address_scaled\")\n",
    "print(\"   - income_year_scaled\")\n",
    "print(\"\\n3. Variables escaladas con StandardScaler:\")\n",
    "print(\"   - tenure_scaled\")\n",
    "print(\"   - age_scaled\")\n",
    "print(\"\\n4. Variables derivadas:\")\n",
    "print(\"   - income_per_year_employed\")\n",
    "print(\"   - stability_score\")\n",
    "print(\"\\n5. Variables dummy:\")\n",
    "print(\"   - region_1, region_2, region_3\")\n",
    "\n",
    "# 8. Identificar correlaciones altas\n",
    "print(\"\\nCorrelaciones significativas (>0.7):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            print(f\"{correlation_matrix.columns[i]} - {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación final para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def prepare_final_features(df_processed, handle_multicollinearity=True):\n",
    "    \"\"\"\n",
    "    Prepara las características finales para modelado, manejando\n",
    "    multicolinealidad y valores faltantes.\n",
    "    \"\"\"\n",
    "    # 1. Manejar el valor faltante en stability_score\n",
    "    df_processed['stability_score'] = df_processed['stability_score'].fillna(\n",
    "        df_processed['stability_score'].median()\n",
    "    )\n",
    "\n",
    "    # 2. Definir características base\n",
    "    base_features = [\n",
    "        'income_log',\n",
    "        'employ_scaled',\n",
    "        'age_scaled',\n",
    "        'income_year_scaled',\n",
    "        'region_1', 'region_2', 'region_3'\n",
    "    ]\n",
    "\n",
    "    # 3. Manejar multicolinealidad\n",
    "    if handle_multicollinearity:\n",
    "        # Aplicar PCA a las variables correlacionadas\n",
    "        correlated_features = ['address_scaled', 'tenure_scaled', 'stability_score']\n",
    "        pca = PCA(n_components=1)\n",
    "        stability_component = pca.fit_transform(\n",
    "            df_processed[correlated_features]\n",
    "        )\n",
    "\n",
    "        # Añadir componente PCA al dataset\n",
    "        df_processed['stability_index'] = stability_component\n",
    "        final_features = base_features + ['stability_index']\n",
    "    else:\n",
    "        # Usar todas las características\n",
    "        final_features = base_features + ['address_scaled', 'tenure_scaled', 'stability_score']\n",
    "\n",
    "    # 4. Preparar X e y\n",
    "    X = df_processed[final_features]\n",
    "    y = df_processed['custcat']\n",
    "\n",
    "    # 5. Split estratificado\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, final_features\n",
    "\n",
    "# Ejemplo de uso\n",
    "X_train, X_test, y_train, y_test, features = prepare_final_features(df_processed)\n",
    "\n",
    "# Mostrar información del proceso\n",
    "print(\"Características finales seleccionadas:\")\n",
    "print(features)\n",
    "print(\"\\nDimensiones de los conjuntos:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(\"\\nDistribución de clases en train:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True).round(3))\n",
    "print(\"\\nDistribución de clases en test:\")\n",
    "print(pd.Series(y_test).value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def create_improved_pipeline():\n",
    "    \"\"\"\n",
    "    Pipeline mejorado con más opciones y mejor preprocesamiento\n",
    "    \"\"\"\n",
    "    # 1. Crear pipeline con preprocesamiento\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # 2. Parámetros más exhaustivos\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.3],\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__min_samples_split': [2, 5],\n",
    "        'classifier__subsample': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    # 3. GridSearch con métricas más robustas\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "def evaluate_model_detailed(model, X_train, X_test, y_train, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Evaluación más detallada del modelo\n",
    "    \"\"\"\n",
    "    # Entrenar modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 1. Visualizaciones\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Matriz de confusión normalizada\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Matriz de Confusión Normalizada')\n",
    "\n",
    "    # Importancia de características\n",
    "    if hasattr(model, 'best_estimator_'):\n",
    "        importances = model.best_estimator_.named_steps['classifier'].feature_importances_\n",
    "    else:\n",
    "        importances = model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=True)\n",
    "\n",
    "    sns.barplot(x='importance', y='feature', data=importance_df, ax=axes[0,1])\n",
    "    axes[0,1].set_title('Importancia de Características')\n",
    "\n",
    "    # Predicciones por clase\n",
    "    class_probs = model.predict_proba(X_test)\n",
    "    avg_probs = np.mean(class_probs, axis=0)\n",
    "    sns.barplot(x=range(len(avg_probs)), y=avg_probs, ax=axes[1,0])\n",
    "    axes[1,0].set_title('Probabilidad Media por Clase')\n",
    "\n",
    "    # Error por clase\n",
    "    errors = y_test != y_pred\n",
    "    error_by_class = pd.DataFrame({'error': errors, 'class': y_test})\n",
    "    sns.barplot(x='class', y='error', data=error_by_class, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Tasa de Error por Clase')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Métricas detalladas\n",
    "    print(\"\\nReporte de Clasificación Detallado:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    if hasattr(model, 'best_params_'):\n",
    "        print(\"\\nMejores parámetros:\")\n",
    "        print(model.best_params_)\n",
    "\n",
    "        print(\"\\nMejor score en cross-validation:\", model.best_score_)\n",
    "\n",
    "    # 3. Análisis de confianza de predicciones\n",
    "    probs = model.predict_proba(X_test)\n",
    "    max_probs = np.max(probs, axis=1)\n",
    "\n",
    "    print(\"\\nConfianza media de predicciones:\", np.mean(max_probs))\n",
    "    print(\"Confianza mínima:\", np.min(max_probs))\n",
    "    print(\"Confianza máxima:\", np.max(max_probs))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Ejecutar el pipeline mejorado\n",
    "print(\"Entrenando modelo mejorado...\")\n",
    "improved_model = create_improved_pipeline()\n",
    "trained_improved_model = evaluate_model_detailed(\n",
    "    improved_model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    features\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
